{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "from tqdm.auto import tqdm  # Import tqdm for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343597383\n"
     ]
    }
   ],
   "source": [
    "print_interval = 60\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "# Get total system memory\n",
    "total_memory = psutil.virtual_memory().total  # in bytes\n",
    "\n",
    "# Set initial chunk size (e.g., 1% of total memory)\n",
    "chunk_size = total_memory // 100\n",
    "print(chunk_size)\n",
    "# Initialize the total rows read\n",
    "total_rows_read = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read admission, diagnosis_icd (icd codes given to each patient each stay), and d_idc_diagnosis (disease name for each code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of admission records in CSV File\n",
      "431231\n"
     ]
    }
   ],
   "source": [
    "# admissions\n",
    "admission=pd.read_csv('admissions.csv', engine='python', on_bad_lines='warn')\n",
    "print(\"The number of admission records in CSV File\")\n",
    "print (len(admission)) #431231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of diagnoses given to all patients\n",
      "4756326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ICD codes given to each patient\n",
    "patient_icd_diagnosis=pd.read_csv('diagnoses_icd-2.csv', engine='python', on_bad_lines='warn')\n",
    "print(\"The number of diagnoses given to all patients\")\n",
    "print (len(patient_icd_diagnosis)) #4756326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of icd_diagnoses\n",
      "109775\n"
     ]
    }
   ],
   "source": [
    "# Disease names for each ICD code\n",
    "disease_icd=pd.read_csv('d_icd_diagnoses.csv', engine='python', on_bad_lines='warn')\n",
    "print(\"The number of icd_diagnoses\")\n",
    "print (len(disease_icd)) #109775"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging & Filtering Some Of The Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# admissions and patient icd codes\n",
    "merge_tables=pd.merge(admission, patient_icd_diagnosis, on=['subject_id', 'hadm_id'], how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acute kidney failure, unspecified ICD code = 5849\n",
    "# Diabetes mellitus without mention of complication, Type II or unspecified ICD code = 25000\n",
    "filter_merged_tables=merge_tables[(merge_tables['icd_code']== '5849') | (merge_tables['icd_code']== '25000')] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease names for each code\n",
    "filters_tables=pd.merge(filter_merged_tables, disease_icd, on=['icd_code', 'icd_version'], how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of database\n",
      "   subject_id   hadm_id            admittime            dischtime deathtime  \\\n",
      "0    10000635  26134563  2136-06-19 14:24:00  2136-06-20 11:30:00       NaN   \n",
      "1    10000980  24947999  2190-11-06 20:57:00  2190-11-08 15:58:00       NaN   \n",
      "\n",
      "           admission_type admit_provider_id admission_location  \\\n",
      "0  AMBULATORY OBSERVATION            P611A0     PROCEDURE SITE   \n",
      "1                EW EMER.            P434W4     EMERGENCY ROOM   \n",
      "\n",
      "  discharge_location insurance language marital_status  \\\n",
      "0                NaN     Other  ENGLISH        WIDOWED   \n",
      "1   HOME HEALTH CARE  Medicare  ENGLISH        MARRIED   \n",
      "\n",
      "                     race            edregtime            edouttime  \\\n",
      "0  BLACK/AFRICAN AMERICAN                  NaN                  NaN   \n",
      "1  BLACK/AFRICAN AMERICAN  2190-11-06 15:30:00  2190-11-06 23:16:00   \n",
      "\n",
      "   hospital_expire_flag  seq_num icd_code  icd_version  \\\n",
      "0                     0        2    25000            9   \n",
      "1                     0        3    25000            9   \n",
      "\n",
      "                                          long_title  \n",
      "0  Diabetes mellitus without mention of complicat...  \n",
      "1  Diabetes mellitus without mention of complicat...  \n",
      "Index(['subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime',\n",
      "       'admission_type', 'admit_provider_id', 'admission_location',\n",
      "       'discharge_location', 'insurance', 'language', 'marital_status', 'race',\n",
      "       'edregtime', 'edouttime', 'hospital_expire_flag', 'seq_num', 'icd_code',\n",
      "       'icd_version', 'long_title'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Example of database\")\n",
    "print(filters_tables.head(2))\n",
    "print(filters_tables.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading lab events and presciption files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab Events-read the dataset in chunks\n",
    "\n",
    "all_merged_chunks=pd.DataFrame()\n",
    "\n",
    "for chunk in tqdm(pd.read_csv('labevents.csv', chunksize=chunk_size), desc=\"Processing chunks\"):\n",
    "    # Update total rows read\n",
    "    total_rows_read += len(chunk)\n",
    "    \n",
    "    merged_chunk= pd.merge(filters_tables, chunk, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    all_merged_chunks=pd.concat([all_merged_chunks, merged_chunk])\n",
    "\n",
    "    # Check if it's time to print elapsed time\n",
    "    if time.time() - start_time > print_interval:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        print_interval += 60  # Increase the print interval to every minute for the next print    \n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "# calculate the total elapsed time\n",
    "total_elapsed_time = end_time - start_time\n",
    "print(f\"Total time taken to read {total_rows_read} rows: {total_elapsed_time} seconds\")\n",
    "print(f\"Total number of rows is: {total_rows_read}\") #?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb83c2da76c8415184b59be30e2bc540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m merged_chunk\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(filters_tables, chunk_df, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhadm_id\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m all_merged_chunks\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mconcat([all_merged_chunks, merged_chunk])\n\u001b[0;32m---> 15\u001b[0m \u001b[43mpbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# NB: don't `finally: close()`\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# since this could be a shared bar which the user will `reset()`\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39mn)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lab Events option 2-read the dataset in chunks\n",
    "\n",
    "all_merged_chunks=pd.DataFrame()\n",
    "\n",
    "with tqdm(total=100) as pbar:  # Initialize tqdm progress bar\n",
    "    while True:\n",
    "        # Read a chunk of the dataset\n",
    "        chunk = pd.read_csv('labevents.csv', chunksize=chunk_size) \n",
    "        for chunk_df in chunk:\n",
    "            # Update total rows read\n",
    "            total_rows_read += len(chunk_df)\n",
    "        \n",
    "            merged_chunk= pd.merge(filters_tables, chunk_df, on=['subject_id', 'hadm_id'], how='inner')\n",
    "            all_merged_chunks=pd.concat([all_merged_chunks, merged_chunk])\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab Items-read the dataset in chunks\n",
    "\n",
    "lab_merged_chunks=pd.DataFrame()\n",
    "\n",
    "for chunk in tqdm(pd.read_csv('d_labitems.csv', chunksize=chunk_size), desc=\"Processing chunks\"):\n",
    "    # Update total rows read\n",
    "    total_rows_read += len(chunk)\n",
    "    \n",
    "    merged_chunk= pd.merge(all_merged_chunks, chunk, on=['itemid'], how='inner')\n",
    "    lab_merged_chunks=pd.concat([lab_merged_chunks, merged_chunk])\n",
    "\n",
    "    # Check if it's time to print elapsed time\n",
    "    if time.time() - start_time > print_interval:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        print_interval += 60  # Increase the print interval to every minute for the next print    \n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "# calculate the total elapsed time\n",
    "total_elapsed_time = end_time - start_time\n",
    "print(f\"Total time taken to read {total_rows_read} rows: {total_elapsed_time} seconds\")\n",
    "print(f\"Total number of rows is: {total_rows_read}\") #?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lab items option 2\n",
    "\n",
    "# Lab Items-read the dataset in chunks\n",
    "lab_merged_chunks=pd.DataFrame()\n",
    "\n",
    "with tqdm(total=100) as pbar:  # Initialize tqdm progress bar\n",
    "    while True:\n",
    "        # Read a chunk of the dataset\n",
    "        chunk = pd.read_csv('d_labitems.csv', chunksize=chunk_size) \n",
    "        for chunk_df in chunk:\n",
    "            # Update total rows read\n",
    "            total_rows_read += len(chunk_df)\n",
    "    \n",
    "            merged_chunk= pd.merge(all_merged_chunks, chunk_df, on=['itemid'], how='inner')\n",
    "            lab_merged_chunks=pd.concat([lab_merged_chunks, merged_chunk])\n",
    "            pbar.update(1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prescriptions option 2- read the dataset in chunks\n",
    "p_merged_chunks=pd.DataFrame()\n",
    "\n",
    "with tqdm(total=100) as pbar:  # Initialize tqdm progress bar\n",
    "    while True:\n",
    "        # Read a chunk of the dataset\n",
    "        chunk = pd.read_csv('prescriptions.csv', chunksize=chunk_size) \n",
    "        for chunk_df in chunk:\n",
    "            # Update total rows read\n",
    "            total_rows_read += len(chunk_df)\n",
    "\n",
    "            merged_chunk= pd.merge(lab_merged_chunks, chunk_df, on=['subject_id', 'hadm_id'], how='inner')\n",
    "            p_merged_chunks=pd.concat([p_merged_chunks, merged_chunk])\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prescriptions- read the dataset in chunks\n",
    "p_merged_chunks=pd.DataFrame()\n",
    "\n",
    "for chunk in tqdm(pd.read_csv('prescriptions.csv', chunksize=chunk_size), desc=\"Processing chunks\"):\n",
    "    # Update total rows read\n",
    "    total_rows_read += len(chunk)\n",
    "    merged_chunk= pd.merge(lab_merged_chunks, chunk, on=['subject_id', 'hadm_id'], how='inner')\n",
    "    p_merged_chunks=pd.concat([p_merged_chunks, merged_chunk])\n",
    "\n",
    "    # Check if it's time to print elapsed time\n",
    "    if time.time() - start_time > print_interval:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        print_interval += 60  # Increase the print interval to every minute for the next print    \n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "# calculate the total elapsed time\n",
    "total_elapsed_time = end_time - start_time\n",
    "print(f\"Total time taken to read {total_rows_read} rows: {total_elapsed_time} seconds\")\n",
    "print(f\"Total number of rows is: {total_rows_read}\") #?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of database\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Example of database\")\n",
    "print(all_merged_chunks.head(2))\n",
    "print(all_merged_chunks.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - First 20,000 rows. Save to CSV File\n",
    "train_records=filter_merged_tables.iloc[:20000]\n",
    "#train_records.loc[:,'DIAGNOSIS']=train_records['DIAGNOSIS'].apply(lambda a: a if a in most_common else 'Other')\n",
    "train_records.to_csv('MIMIC_IV_train.csv', index=False) \n",
    "print(\"File saved.\") \n",
    "print(len(train_records))      \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation - 10,000 rows. Save to CSV File\n",
    "validation_records=merge_tables.iloc[20000:30000]\n",
    "#validation_records.loc[:,'DIAGNOSIS']=validation_records['DIAGNOSIS'].apply(lambda a: a if a in most_common else 'Other')\n",
    "validation_records.to_csv('MIMIC_IV_validation.csv', index=False)\n",
    "print(\"File saved\")\n",
    "print(len(validation_records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - 5,000 rows. Save to CSV File\n",
    "test_records=merge_tables.iloc[30000:35000]\n",
    "#test_records.loc[:,'DIAGNOSIS']=test_records['DIAGNOSIS'].apply(lambda a: a if a in most_common else 'Other')\n",
    "test_records.to_csv('MIMIC_IV_validation.csv', index=False)\n",
    "print(\"File saved\")\n",
    "print(len(test_records))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
