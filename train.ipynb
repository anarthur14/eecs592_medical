{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7b8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, callbacks\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from fairlearn.metrics import equalized_odds_difference, equalized_odds_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5dd4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir = './pipeline/cleaned/'\n",
    "\n",
    "cohort = pd.read_csv(read_dir + 'cohort.csv', index_col=0)\n",
    "\n",
    "diag = pd.read_csv(read_dir + 'diag.csv', index_col=0)\n",
    "diag = diag.set_index('hadm_id')\n",
    "\n",
    "labs = pd.read_csv(read_dir + 'labs.csv', index_col=0)\n",
    "labs = labs.set_index('hadm_id')\n",
    "\n",
    "proc = pd.read_csv(read_dir + 'proc.csv', index_col=0)\n",
    "proc = proc.set_index('hadm_id')\n",
    "\n",
    "meds = pd.read_csv(read_dir + 'meds.csv', index_col=0)\n",
    "meds = meds.set_index('hadm_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9163ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf):\n",
    "    if hf:\n",
    "        split = 'split_hf'\n",
    "    else:\n",
    "        split = 'split_diabetes'\n",
    "    train_hadm = cohort[cohort[split]=='train'].hadm_id.to_numpy()\n",
    "    valid_hadm = cohort[cohort[split]=='validation'].hadm_id.to_numpy()\n",
    "    test_hadm = cohort[cohort[split]=='test'].hadm_id.to_numpy()\n",
    "    train_hadm = np.sort(train_hadm)\n",
    "    valid_hadm = np.sort(valid_hadm)\n",
    "    test_hadm = np.sort(test_hadm)\n",
    "    model = Transformer(lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf)\n",
    "    print(model.model.summary())\n",
    "    return model, train_hadm, valid_hadm, test_hadm\n",
    "\n",
    "\n",
    "class Transformer(object):\n",
    "    def __init__(self, lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf):\n",
    "        keras.backend.clear_session()\n",
    "        physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "        #print(f\"GPU list {physical_devices}\")\n",
    "        #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        self.opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.loss = keras.losses.BinaryCrossentropy()\n",
    "        self.metric = keras.metrics.AUC(name='auc')\n",
    "        \n",
    "        self.cohort = cohort\n",
    "        self.diag = diag.copy()\n",
    "        self.labs = labs\n",
    "        self.proc = proc\n",
    "        self.meds = meds\n",
    "        \n",
    "        self.use_diag = use_diag\n",
    "        self.use_labs = use_labs\n",
    "        self.use_proc = use_proc\n",
    "        self.use_meds = use_meds\n",
    "        self.use_insurance = use_insurance\n",
    "        self.use_gender = use_gender\n",
    "        self.use_race = use_race\n",
    "        \n",
    "        self.input_shapes = self.get_input_shapes()\n",
    "        if hf:\n",
    "            self.label = 'label_hf'\n",
    "            self.diag['I50'] = 0\n",
    "        else:\n",
    "            self.label = 'label_diabetes'\n",
    "            self.diag['E11'] = 0\n",
    "         \n",
    "        self.buildModel()\n",
    "    \n",
    "    def embedding(self, inputs):\n",
    "        maxlen = inputs.shape[-2]\n",
    "        varnum = inputs.shape[-1]\n",
    "        pos = keras.backend.arange(start=0, stop=maxlen, step=1)\n",
    "        pos = layers.Embedding(input_dim=maxlen, output_dim=256)(pos)\n",
    "        x = layers.Dense(units=256)(inputs)\n",
    "        return x + pos\n",
    "    \n",
    "    def encoder(self, inputs):\n",
    "        hidden_dim = inputs.shape[-1]\n",
    "        if len(inputs.shape) == 2:\n",
    "            y = layers.Dense(hidden_dim)(inputs)\n",
    "            y = layers.Dropout(0.1)(y)\n",
    "            y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "            return tf.expand_dims(y, 1)\n",
    "        \n",
    "        x = layers.MultiHeadAttention(num_heads=8, key_dim=hidden_dim)(inputs, inputs)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "        \n",
    "        y = layers.Dense(hidden_dim, activation='relu')(x)\n",
    "        y = layers.Dense(hidden_dim)(y)\n",
    "        y = layers.Dropout(0.1)(y)\n",
    "        y = layers.LayerNormalization(epsilon=1e-6)(y + x)\n",
    "        return y\n",
    "    \n",
    "    def buildModel(self): \n",
    "        inputs = [keras.Input(shape=shape) for shape in self.input_shapes]\n",
    "        encoded = [self.encoder(inp) for inp in inputs]\n",
    "        if len(inputs) > 1:\n",
    "            combined = layers.Concatenate(axis=-1)([layers.GlobalAveragePooling1D()(enc) for enc in encoded])\n",
    "        else:\n",
    "            combined = layers.GlobalAveragePooling1D()(encoded[0])\n",
    "        x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        self.model.compile(optimizer=self.opt, loss=self.loss, metrics=[self.metric])\n",
    "    \n",
    "    def train(self, train_hadm, valid_hadm, epochs, batchSize, path, val_auc=False, verbose_ckp=1, verbose_fit=2):\n",
    "        if val_auc:\n",
    "            MCP = callbacks.ModelCheckpoint(path, monitor='val_auc', verbose=verbose_ckp, save_best_only=True, mode='max')\n",
    "            ES = callbacks.EarlyStopping(monitor='val_auc', patience=10, verbose=verbose_ckp, mode='max')\n",
    "        else:\n",
    "            MCP = callbacks.ModelCheckpoint(path, monitor='val_loss', verbose=verbose_ckp, save_best_only=True, mode='min') \n",
    "            ES = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=verbose_ckp, mode='min')\n",
    "        train_gen = self.generator(train_hadm, batchSize)\n",
    "        valid_gen = self.generator(valid_hadm, batchSize)\n",
    "        self.model.fit(train_gen, validation_data=valid_gen, epochs=epochs, verbose=verbose_fit, callbacks=[MCP, ES], \n",
    "                       steps_per_epoch=len(train_hadm)//batchSize, validation_steps=len(valid_hadm)//batchSize)\n",
    "        self.model = models.load_model(path, compile=False)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = models.load_model(path, compile=False)\n",
    "        \n",
    "    def predict(self, test_hadm, batchSize, verbose=2):\n",
    "        pred_gen = self.pred_generator(test_hadm, batchSize)\n",
    "        pred = self.model.predict(pred_gen, verbose=verbose, steps=(len(test_hadm)+batchSize-1)//batchSize)\n",
    "        return pred\n",
    "\n",
    "    def generator(self, input_hadm, batchSize):\n",
    "        while 1:\n",
    "            hadm_ids = np.sort(np.random.choice(input_hadm, batchSize, False))\n",
    "            x = self.get_x(hadm_ids, batchSize)\n",
    "            y = self.get_y(hadm_ids)\n",
    "            yield x, y\n",
    "            \n",
    "    def pred_generator(self, input_hadm, batchSize):\n",
    "        i = 0\n",
    "        while 1:\n",
    "            hadm_ids = input_hadm[np.arange(i, min(i+batchSize, len(input_hadm)))]\n",
    "            if i+batchSize >= len(input_hadm):\n",
    "                i = 0\n",
    "            i += len(hadm_ids)\n",
    "            x = self.get_x(hadm_ids, len(hadm_ids))\n",
    "            y = self.get_y(hadm_ids)\n",
    "            yield x, y\n",
    "            \n",
    "    def get_y(self, hadm_ids):\n",
    "        batch_cohort = self.cohort[self.cohort.hadm_id.isin(hadm_ids)].sort_values('hadm_id')\n",
    "        y = batch_cohort[self.label].to_numpy()\n",
    "        return y\n",
    "\n",
    "    def get_input_shapes(self):\n",
    "        xs = []\n",
    "        xd = ( 3, 306)\n",
    "        xl = (14, 107)\n",
    "        xp = ( 2,  18)\n",
    "        xm = (14,  55)\n",
    "    \n",
    "        if self.use_diag:\n",
    "            xs.append(xd)\n",
    "\n",
    "        if self.use_labs:\n",
    "            xs.append(xl)\n",
    "\n",
    "        if self.use_proc:\n",
    "            xs.append(xp)\n",
    "            \n",
    "        if self.use_meds:\n",
    "            xs.append(xm)\n",
    "            \n",
    "        cohort_columns = []\n",
    "        if self.use_insurance:\n",
    "            cohort_columns.append('medicare')\n",
    "            cohort_columns.append('medicaid')\n",
    "        if self.use_gender:\n",
    "            cohort_columns.append('male')\n",
    "        if self.use_race:\n",
    "            cohort_columns.append('white')\n",
    "        if len(cohort_columns) > 0:\n",
    "            xs.append((len(cohort_columns),))\n",
    "\n",
    "        return xs\n",
    "    \n",
    "    def get_x(self, hadm_ids, batchSize):\n",
    "        batch_cohort = self.cohort[self.cohort.hadm_id.isin(hadm_ids)].sort_values('hadm_id')\n",
    "\n",
    "        xs = []\n",
    "        xd = np.zeros((batchSize,  3, 306))\n",
    "        xl = np.zeros((batchSize, 14, 107))\n",
    "        xp = np.zeros((batchSize,  2,  18))\n",
    "        xm = np.zeros((batchSize, 14,  55))\n",
    "\n",
    "        if self.use_diag:\n",
    "            for i in range(batchSize):\n",
    "                seqnum = min(xd.shape[1], batch_cohort.seqnum.iloc[i]+1)\n",
    "                raw_index = batch_cohort.index[i]\n",
    "                xd[i, 0:seqnum] = self.diag.loc[self.cohort[(raw_index-seqnum+1):(raw_index+1)].hadm_id.to_numpy()].to_numpy()\n",
    "            xs.append(xd)\n",
    "\n",
    "        if self.use_labs:\n",
    "            for i in range(batchSize):\n",
    "                hadm_id = hadm_ids[i]\n",
    "                if hadm_id in self.labs.index:\n",
    "                    labs_i = self.labs.loc[hadm_id:hadm_id+1]\n",
    "                    seqnum = min(xl.shape[1], len(labs_i))\n",
    "                    xl[i, 0:seqnum] = labs_i.iloc[-seqnum:].to_numpy()\n",
    "            xs.append(xl)\n",
    "\n",
    "        if self.use_proc:\n",
    "            for i in range(batchSize):\n",
    "                hadm_id = hadm_ids[i]\n",
    "                if hadm_id in self.proc.index:\n",
    "                    proc_i = self.proc.loc[hadm_id:hadm_id+1]\n",
    "                    seqnum = min(xp.shape[1], len(proc_i))\n",
    "                    xp[i, 0:seqnum] = proc_i.iloc[-seqnum:].to_numpy()\n",
    "            xs.append(xp)\n",
    "\n",
    "        if self.use_meds:\n",
    "            for i in range(batchSize):\n",
    "                hadm_id = hadm_ids[i]\n",
    "                if hadm_id in self.meds.index:\n",
    "                    meds_i = self.meds.loc[hadm_id:hadm_id+1]\n",
    "                    seqnum = min(xm.shape[1], len(meds_i))\n",
    "                    xm[i, 0:seqnum] = meds_i.iloc[-seqnum:].to_numpy()\n",
    "            xs.append(xm)\n",
    "\n",
    "        cohort_columns = []\n",
    "        if self.use_insurance:\n",
    "            cohort_columns.append('medicare')\n",
    "            cohort_columns.append('medicaid')\n",
    "        if self.use_gender:\n",
    "            cohort_columns.append('male')\n",
    "        if self.use_race:\n",
    "            cohort_columns.append('white')\n",
    "        if len(cohort_columns) > 0:\n",
    "            xs.append(batch_cohort[cohort_columns].to_numpy())\n",
    "\n",
    "        return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826e510",
   "metadata": {},
   "source": [
    "# All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe8e0e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 3, 306)]             0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 14, 107)]            0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 2, 18)]              0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 14, 55)]             0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 3, 306)               3004002   ['input_1[0][0]',             \n",
      " iHeadAttention)                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 14, 107)              369043    ['input_2[0][0]',             \n",
      " ltiHeadAttention)                                                   'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 2, 18)                10818     ['input_3[0][0]',             \n",
      " ltiHeadAttention)                                                   'input_3[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 14, 55)               98175     ['input_4[0][0]',             \n",
      " ltiHeadAttention)                                                   'input_4[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 3, 306)               0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 14, 107)              0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 2, 18)                0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 14, 55)               0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 3, 306)               0         ['dropout[0][0]',             \n",
      " Lambda)                                                             'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 14, 107)              0         ['dropout_2[0][0]',           \n",
      " OpLambda)                                                           'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 2, 18)                0         ['dropout_4[0][0]',           \n",
      " OpLambda)                                                           'input_3[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 14, 55)               0         ['dropout_6[0][0]',           \n",
      " OpLambda)                                                           'input_4[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 3, 306)               612       ['tf.__operators__.add[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 14, 107)              214       ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 2, 18)                36        ['tf.__operators__.add_4[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 14, 55)               110       ['tf.__operators__.add_6[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 3, 306)               93942     ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 14, 107)              11556     ['layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 2, 18)                342       ['layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 14, 55)               3080      ['layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 4)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 3, 306)               93942     ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 14, 107)              11556     ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 2, 18)                342       ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 14, 55)               3080      ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 4)                    20        ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 3, 306)               0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 14, 107)              0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 2, 18)                0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 14, 55)               0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 4)                    0         ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 3, 306)               0         ['dropout_1[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 14, 107)              0         ['dropout_3[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 2, 18)                0         ['dropout_5[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 14, 55)               0         ['dropout_7[0][0]',           \n",
      " OpLambda)                                                           'layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_8 (Lay  (None, 4)                    8         ['dropout_8[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 3, 306)               612       ['tf.__operators__.add_1[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 14, 107)              214       ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 2, 18)                36        ['tf.__operators__.add_5[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 14, 55)               110       ['tf.__operators__.add_7[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda  (None, 1, 4)                 0         ['layer_normalization_8[0][0]'\n",
      " )                                                                  ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 306)                  0         ['layer_normalization_1[0][0]'\n",
      " GlobalAveragePooling1D)                                            ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 107)                  0         ['layer_normalization_3[0][0]'\n",
      "  (GlobalAveragePooling1D)                                          ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 18)                   0         ['layer_normalization_5[0][0]'\n",
      "  (GlobalAveragePooling1D)                                          ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 55)                   0         ['layer_normalization_7[0][0]'\n",
      "  (GlobalAveragePooling1D)                                          ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 4)                    0         ['tf.expand_dims[0][0]']      \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 490)                  0         ['global_average_pooling1d[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'global_average_pooling1d_1[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'global_average_pooling1d_2[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'global_average_pooling1d_3[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'global_average_pooling1d_4[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 64)                   31424     ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 1)                    65        ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3733339 (14.24 MB)\n",
      "Trainable params: 3733339 (14.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "use_diag = 1\n",
    "use_labs = 1\n",
    "use_proc = 1\n",
    "use_meds = 1\n",
    "use_insurance = 1\n",
    "use_gender = 1\n",
    "use_race = 1\n",
    "hf = 0\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "batchSize = 256\n",
    "path='./models/all.h5'\n",
    "\n",
    "# Initialize Model\n",
    "model, train_hadm, valid_hadm, test_hadm = initialize(lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5bd660bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4769 - auc: 0.6642\n",
      "Epoch 1: val_loss improved from inf to 0.44444, saving model to ./models/all.h5\n",
      "91/91 [==============================] - 160s 2s/step - loss: 0.4769 - auc: 0.6642 - val_loss: 0.4444 - val_auc: 0.7119\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - ETA: 0s - loss: 0.4223 - auc: 0.7448\n",
      "Epoch 2: val_loss improved from 0.44444 to 0.44168, saving model to ./models/all.h5\n",
      "91/91 [==============================] - 154s 2s/step - loss: 0.4223 - auc: 0.7448 - val_loss: 0.4417 - val_auc: 0.7294\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - ETA: 0s - loss: 0.4209 - auc: 0.7535\n",
      "Epoch 3: val_loss improved from 0.44168 to 0.42882, saving model to ./models/all.h5\n",
      "91/91 [==============================] - 160s 2s/step - loss: 0.4209 - auc: 0.7535 - val_loss: 0.4288 - val_auc: 0.7327\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - ETA: 0s - loss: 0.4138 - auc: 0.7696\n",
      "Epoch 4: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 169s 2s/step - loss: 0.4138 - auc: 0.7696 - val_loss: 0.4511 - val_auc: 0.7095\n",
      "Epoch 5/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3911 - auc: 0.7948\n",
      "Epoch 5: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 210s 2s/step - loss: 0.3911 - auc: 0.7948 - val_loss: 0.4407 - val_auc: 0.7070\n",
      "Epoch 6/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3730 - auc: 0.8107\n",
      "Epoch 6: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 169s 2s/step - loss: 0.3730 - auc: 0.8107 - val_loss: 0.4433 - val_auc: 0.7265\n",
      "Epoch 7/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3698 - auc: 0.8202\n",
      "Epoch 7: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 193s 2s/step - loss: 0.3698 - auc: 0.8202 - val_loss: 0.4655 - val_auc: 0.7173\n",
      "Epoch 8/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3557 - auc: 0.8341\n",
      "Epoch 8: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 198s 2s/step - loss: 0.3557 - auc: 0.8341 - val_loss: 0.4693 - val_auc: 0.7256\n",
      "Epoch 9/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3452 - auc: 0.8517\n",
      "Epoch 9: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 192s 2s/step - loss: 0.3452 - auc: 0.8517 - val_loss: 0.4879 - val_auc: 0.6836\n",
      "Epoch 10/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3220 - auc: 0.8744\n",
      "Epoch 10: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 181s 2s/step - loss: 0.3220 - auc: 0.8744 - val_loss: 0.5551 - val_auc: 0.6914\n",
      "Epoch 11/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.3031 - auc: 0.8904\n",
      "Epoch 11: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 183s 2s/step - loss: 0.3031 - auc: 0.8904 - val_loss: 0.5642 - val_auc: 0.6708\n",
      "Epoch 12/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.2715 - auc: 0.9127\n",
      "Epoch 12: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 161s 2s/step - loss: 0.2715 - auc: 0.9127 - val_loss: 0.6252 - val_auc: 0.6672\n",
      "Epoch 13/100\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.2468 - auc: 0.9292\n",
      "Epoch 13: val_loss did not improve from 0.42882\n",
      "91/91 [==============================] - 167s 2s/step - loss: 0.2468 - auc: 0.9292 - val_loss: 0.5981 - val_auc: 0.6821\n",
      "Epoch 13: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model.train(train_hadm, valid_hadm, epochs, batchSize, path, False, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34c13329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness(bias, m, y_true, res):\n",
    "    #y_true = cohort.set_index('hadm_id').loc[m]['label_diabetes'].to_numpy()\n",
    "    #pred = model.predict(train_hadm, batchSize, 1)\n",
    "  \n",
    "    bias_labels = []\n",
    "    for hadm_id in m:\n",
    "        patient_data = cohort.loc[cohort['hadm_id'] == hadm_id]\n",
    "        bias_labels.append(patient_data[bias].iloc[0])  \n",
    "\n",
    "    # use mean has threshold\n",
    "    threshold = np.mean(res)\n",
    "\n",
    "    # convert res into binary prediction\n",
    "    binary_pred = [1 if x > threshold else 0 for x in res]\n",
    "\n",
    "    eod_diff= equalized_odds_difference(y_true, binary_pred, sensitive_features=bias_labels)\n",
    "    eod_ratio = equalized_odds_ratio(y_true, binary_pred, sensitive_features=bias_labels)\n",
    "\n",
    "    return eod_diff, eod_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Results\n",
    "model.load(path)\n",
    "res = model.predict(train_hadm, batchSize, 1)\n",
    "y_true = cohort.set_index('hadm_id').loc[train_hadm].label_diabetes.to_numpy()\n",
    "auc=roc_auc_score(y_true, res)\n",
    "prec=average_precision_score(y_true, res)\n",
    "print(f'AUC Score: {auc}')\n",
    "print(f'AUPCR Score: {prec}')\n",
    "\n",
    "m='train_hadm'\n",
    "race='white'\n",
    "diff, ratio=fairness(race, m, y_true, res)\n",
    "print(\"Equalized Odds Difference (Race Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Race Parity):\", ratio)\n",
    "\n",
    "gender='male'\n",
    "diff, ratio=fairness(gender, m,  y_true, res) \n",
    "print(\"Equalized Odds Difference (Gender Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Gender Parity):\", ratio)\n",
    "\n",
    "insurance='medicaid'\n",
    "diff, ratio=fairness(insurance, m,  y_true, res)\n",
    "print(\"Equalized Odds Difference (Medicaid Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Medicaid Parity):\", ratio)\n",
    "\n",
    "insurance='medicare'\n",
    "diff, ratio=fairness(insurance, m,  y_true, res)\n",
    "print(\"Equalized Odds Difference (Medicare Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Medicare Parity):\", ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eaf582c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 37s 1s/step\n",
      "AUC Score: 0.7199627511569392\n",
      "AUPCR Score: 0.37025478819726676\n",
      "Equalized Odds Difference (Race Parity): 0.11650234472901838\n",
      "Equalized Odds Ratio (Race Parity): 0.7407183345851188\n",
      "Equalized Odds Difference (Gender Parity): 0.07309971749666877\n",
      "Equalized Odds Ratio (Gender Parity): 0.8117369639588127\n",
      "Equalized Odds Difference (Medicaid Parity): 0.14152220189059628\n",
      "Equalized Odds Ratio (Medicaid Parity): 0.7104736633077878\n",
      "Equalized Odds Difference (Medicare Parity): 0.0500956558256731\n",
      "Equalized Odds Ratio (Medicare Parity): 0.927552466428234\n"
     ]
    }
   ],
   "source": [
    "# Validation Results\n",
    "model.load(path)\n",
    "res = model.predict(valid_hadm, batchSize, 1)\n",
    "y_true = cohort.set_index('hadm_id').loc[valid_hadm].label_diabetes.to_numpy()\n",
    "roc_auc_score(y_true, res), average_precision_score(y_true, res)\n",
    "\n",
    "auc=roc_auc_score(y_true, res)\n",
    "prec=average_precision_score(y_true, res)\n",
    "print(f'AUC Score: {auc}')\n",
    "print(f'AUPCR Score: {prec}')\n",
    "\n",
    "race='white'\n",
    "m=valid_hadm\n",
    "diff, ratio=fairness(race, m,  y_true, res)\n",
    "print(\"Equalized Odds Difference (Race Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Race Parity):\", ratio)\n",
    "\n",
    "gender='male'\n",
    "diff, ratio=fairness(gender, m , y_true, res) \n",
    "print(\"Equalized Odds Difference (Gender Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Gender Parity):\", ratio)\n",
    "\n",
    "insurance='medicaid'\n",
    "diff, ratio=fairness(insurance, m,  y_true, res)\n",
    "print(\"Equalized Odds Difference (Medicaid Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Medicaid Parity):\", ratio)\n",
    "\n",
    "insurance='medicare'\n",
    "diff, ratio=fairness(insurance, m,  y_true, res)\n",
    "print(\"Equalized Odds Difference (Medicare Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Medicare Parity):\", ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a2368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 31s 1s/step\n",
      "AUC Score: 0.7173296475013702\n",
      "AUPCR Score: 0.36429435408782884\n"
     ]
    }
   ],
   "source": [
    "# Test Results\n",
    "model.load(path)\n",
    "res = model.predict(test_hadm, batchSize, 1)\n",
    "y_true = cohort.set_index('hadm_id').loc[test_hadm].label_diabetes.to_numpy()\n",
    "roc_auc_score(y_true, res), average_precision_score(y_true, res)\n",
    "\n",
    "auc=roc_auc_score(y_true, res)\n",
    "prec=average_precision_score(y_true, res)\n",
    "print(f'AUC Score: {auc}')\n",
    "print(f'AUPCR Score: {prec}')\n",
    "\n",
    "race='white'\n",
    "m='test_hadm'\n",
    "diff, ratio=fairness(race, m,  y_true, res)\n",
    "print(\"Equalized Odds Difference (Race Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Race Parity):\", ratio)\n",
    "\n",
    "gender='male'\n",
    "diff, ratio=fairness(gender, m , y_true, res) \n",
    "print(\"Equalized Odds Difference (Gender Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Gender Parity):\", ratio)\n",
    "\n",
    "insurance='medicaid'\n",
    "diff, ratio=fairness(insurance, m,  y_true, res)\n",
    "print(\"Equalized Odds Difference (Medicaid Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Medicaid Parity):\", ratio)\n",
    "\n",
    "insurance='medicare'\n",
    "diff, ratio=fairness(insurance, m,  y_true, res)\n",
    "print(\"Equalized Odds Difference (Medicare Parity):\", diff)\n",
    "print(\"Equalized Odds Ratio (Medicare Parity):\", ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
