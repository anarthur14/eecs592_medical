{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7b8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, callbacks\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5dd4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir = './pipeline/cleaned/'\n",
    "\n",
    "cohort = pd.read_csv(read_dir + 'cohort.csv', index_col=0)\n",
    "\n",
    "diag = pd.read_csv(read_dir + 'diag.csv', index_col=0)\n",
    "diag = diag.set_index('hadm_id')\n",
    "\n",
    "labs = pd.read_csv(read_dir + 'labs.csv', index_col=0)\n",
    "labs = labs.set_index('hadm_id')\n",
    "\n",
    "proc = pd.read_csv(read_dir + 'proc.csv', index_col=0)\n",
    "proc = proc.set_index('hadm_id')\n",
    "\n",
    "meds = pd.read_csv(read_dir + 'meds.csv', index_col=0)\n",
    "meds = meds.set_index('hadm_id')\n",
    "\n",
    "train_hadm = cohort[cohort.split=='train'].hadm_id.to_numpy()\n",
    "valid_hadm = cohort[cohort.split=='validation'].hadm_id.to_numpy()\n",
    "test_hadm = cohort[cohort.split=='test'].hadm_id.to_numpy()\n",
    "test_hadm = np.sort(test_hadm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9163ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(object):\n",
    "    def __init__(self, lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf):\n",
    "        keras.backend.clear_session()\n",
    "        physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "        #print(f\"GPU list {physical_devices}\")\n",
    "        #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        self.opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.loss = keras.losses.BinaryCrossentropy()\n",
    "        self.metric = keras.metrics.AUC(name='auc')\n",
    "        \n",
    "        self.cohort = cohort\n",
    "        self.diag = diag.copy()\n",
    "        self.labs = labs\n",
    "        self.proc = proc\n",
    "        self.meds = meds\n",
    "        \n",
    "        self.use_diag = use_diag\n",
    "        self.use_labs = use_labs\n",
    "        self.use_proc = use_proc\n",
    "        self.use_meds = use_meds\n",
    "        self.use_insurance = use_insurance\n",
    "        self.use_gender = use_gender\n",
    "        self.use_race = use_race\n",
    "        \n",
    "        self.input_shapes = self.get_input_shapes()\n",
    "        if hf:\n",
    "            self.label = 'label_hf'\n",
    "            self.diag['I50'] = 0\n",
    "        else:\n",
    "            self.label = 'label_diabetes'\n",
    "            self.diag['E11'] = 0\n",
    "            \n",
    "        self.buildModel()\n",
    "    \n",
    "    def embedding(self, inputs):\n",
    "        maxlen = inputs.shape[-2]\n",
    "        varnum = inputs.shape[-1]\n",
    "        pos = keras.backend.arange(start=0, stop=maxlen, step=1)\n",
    "        pos = layers.Embedding(input_dim=maxlen, output_dim=256)(pos)\n",
    "        x = layers.Dense(units=256)(inputs)\n",
    "        return x + pos\n",
    "    \n",
    "    def encoder(self, inputs):\n",
    "        hidden_dim = inputs.shape[-1]\n",
    "        if len(inputs.shape) == 2:\n",
    "            y = layers.Dense(hidden_dim)(inputs)\n",
    "            y = layers.Dropout(0.1)(y)\n",
    "            y = layers.LayerNormalization(epsilon=1e-6)(y)\n",
    "            return tf.expand_dims(y, 1)\n",
    "        \n",
    "        x = layers.MultiHeadAttention(num_heads=8, key_dim=hidden_dim)(inputs, inputs)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + inputs)\n",
    "        \n",
    "        y = layers.Dense(hidden_dim, activation='relu')(x)\n",
    "        y = layers.Dense(hidden_dim)(y)\n",
    "        y = layers.Dropout(0.1)(y)\n",
    "        y = layers.LayerNormalization(epsilon=1e-6)(y + x)\n",
    "        return y\n",
    "    \n",
    "    def buildModel(self): \n",
    "        inputs = [keras.Input(shape=shape) for shape in self.input_shapes]\n",
    "        encoded = [self.encoder(inp) for inp in inputs]\n",
    "        if len(inputs) > 1:\n",
    "            combined = layers.Concatenate(axis=-1)([layers.GlobalAveragePooling1D()(enc) for enc in encoded])\n",
    "        else:\n",
    "            combined = layers.GlobalAveragePooling1D()(encoded[0])\n",
    "        x = layers.Dense(64, activation=\"relu\")(combined)\n",
    "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        self.model.compile(optimizer=self.opt, loss=self.loss, metrics=[self.metric])\n",
    "    \n",
    "    def train(self, train_hadm, valid_hadm, epochs, batchSize, path, val_auc=False, verbose_ckp=1, verbose_fit=2):\n",
    "        if val_auc:\n",
    "            MCP = callbacks.ModelCheckpoint(path, monitor='val_auc', verbose=verbose_ckp, save_best_only=True, mode='max')\n",
    "            ES = callbacks.EarlyStopping(monitor='val_auc', patience=10, verbose=verbose_ckp, mode='max')\n",
    "        else:\n",
    "            MCP = callbacks.ModelCheckpoint(path, monitor='val_loss', verbose=verbose_ckp, save_best_only=True, mode='min') \n",
    "            ES = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=verbose_ckp, mode='min')\n",
    "        train_gen = self.generator(train_hadm, batchSize)\n",
    "        valid_gen = self.generator(valid_hadm, batchSize)\n",
    "        self.model.fit(train_gen, validation_data=valid_gen, epochs=epochs, verbose=verbose_fit, callbacks=[MCP, ES], \n",
    "                       steps_per_epoch=len(train_hadm)//batchSize, validation_steps=len(valid_hadm)//batchSize)\n",
    "        self.model = models.load_model(path, compile=False)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = models.load_model(path, compile=False)\n",
    "        \n",
    "    def predict(self, test_hadm, batchSize, verbose=2):\n",
    "        pred_gen = self.pred_generator(test_hadm, batchSize)\n",
    "        pred = self.model.predict(pred_gen, verbose=verbose, steps=(len(test_hadm)+batchSize-1)//batchSize)\n",
    "        return pred\n",
    "\n",
    "    def generator(self, input_hadm, batchSize):\n",
    "        while 1:\n",
    "            hadm_ids = np.sort(np.random.choice(input_hadm, batchSize, False))\n",
    "            x = self.get_x(hadm_ids, batchSize)\n",
    "            y = self.get_y(hadm_ids)\n",
    "            yield x, y\n",
    "            \n",
    "    def pred_generator(self, input_hadm, batchSize):\n",
    "        i = 0\n",
    "        while 1:\n",
    "            hadm_ids = input_hadm[np.arange(i, min(i+batchSize, len(input_hadm)))]\n",
    "            if i+batchSize >= len(input_hadm):\n",
    "                i = 0\n",
    "            i += len(hadm_ids)\n",
    "            x = self.get_x(hadm_ids, len(hadm_ids))\n",
    "            y = self.get_y(hadm_ids)\n",
    "            yield x, y\n",
    "            \n",
    "    def get_y(self, hadm_ids):\n",
    "        batch_cohort = self.cohort[self.cohort.hadm_id.isin(hadm_ids)].sort_values('hadm_id')\n",
    "        y = batch_cohort[self.label].to_numpy()\n",
    "        return y\n",
    "\n",
    "    def get_input_shapes(self):\n",
    "        xs = []\n",
    "        xd = ( 3, 306)\n",
    "        xl = (14, 107)\n",
    "        xp = ( 2,  18)\n",
    "        xm = (14,  55)\n",
    "    \n",
    "        if self.use_diag:\n",
    "            xs.append(xd)\n",
    "\n",
    "        if self.use_labs:\n",
    "            xs.append(xl)\n",
    "\n",
    "        if self.use_proc:\n",
    "            xs.append(xp)\n",
    "            \n",
    "        if self.use_meds:\n",
    "            xs.append(xm)\n",
    "            \n",
    "        cohort_columns = []\n",
    "        if self.use_insurance:\n",
    "            cohort_columns.append('medicare')\n",
    "            cohort_columns.append('medicaid')\n",
    "        if self.use_gender:\n",
    "            cohort_columns.append('male')\n",
    "        if self.use_race:\n",
    "            cohort_columns.append('white')\n",
    "        if len(cohort_columns) > 0:\n",
    "            xs.append((len(cohort_columns),))\n",
    "\n",
    "        return xs\n",
    "    \n",
    "    def get_x(self, hadm_ids, batchSize):\n",
    "        batch_cohort = self.cohort[self.cohort.hadm_id.isin(hadm_ids)].sort_values('hadm_id')\n",
    "\n",
    "        xs = []\n",
    "        xd = np.zeros((batchSize,  3, 306))\n",
    "        xl = np.zeros((batchSize, 14, 107))\n",
    "        xp = np.zeros((batchSize,  2,  18))\n",
    "        xm = np.zeros((batchSize, 14,  55))\n",
    "\n",
    "        if self.use_diag:\n",
    "            for i in range(batchSize):\n",
    "                seqnum = min(xd.shape[1], batch_cohort.seqnum.iloc[i]+1)\n",
    "                raw_index = batch_cohort.index[i]\n",
    "                xd[i, 0:seqnum] = self.diag.loc[self.cohort[(raw_index-seqnum+1):(raw_index+1)].hadm_id.to_numpy()].to_numpy()\n",
    "            xs.append(xd)\n",
    "\n",
    "        if self.use_labs:\n",
    "            for i in range(batchSize):\n",
    "                hadm_id = hadm_ids[i]\n",
    "                if hadm_id in self.labs.index:\n",
    "                    labs_i = self.labs.loc[hadm_id:hadm_id+1]\n",
    "                    seqnum = min(xl.shape[1], len(labs_i))\n",
    "                    xl[i, 0:seqnum] = labs_i.iloc[-seqnum:].to_numpy()\n",
    "            xs.append(xl)\n",
    "\n",
    "        if self.use_proc:\n",
    "            for i in range(batchSize):\n",
    "                hadm_id = hadm_ids[i]\n",
    "                if hadm_id in self.proc.index:\n",
    "                    proc_i = self.proc.loc[hadm_id:hadm_id+1]\n",
    "                    seqnum = min(xp.shape[1], len(proc_i))\n",
    "                    xp[i, 0:seqnum] = proc_i.iloc[-seqnum:].to_numpy()\n",
    "            xs.append(xp)\n",
    "\n",
    "        if self.use_meds:\n",
    "            for i in range(batchSize):\n",
    "                hadm_id = hadm_ids[i]\n",
    "                if hadm_id in self.meds.index:\n",
    "                    meds_i = self.meds.loc[hadm_id:hadm_id+1]\n",
    "                    seqnum = min(xm.shape[1], len(meds_i))\n",
    "                    xm[i, 0:seqnum] = meds_i.iloc[-seqnum:].to_numpy()\n",
    "            xs.append(xm)\n",
    "\n",
    "        cohort_columns = []\n",
    "        if self.use_insurance:\n",
    "            cohort_columns.append('medicare')\n",
    "            cohort_columns.append('medicaid')\n",
    "        if self.use_gender:\n",
    "            cohort_columns.append('male')\n",
    "        if self.use_race:\n",
    "            cohort_columns.append('white')\n",
    "        if len(cohort_columns) > 0:\n",
    "            xs.append(batch_cohort[cohort_columns].to_numpy())\n",
    "\n",
    "        return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42510ad0",
   "metadata": {},
   "source": [
    "# Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5d21f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 14, 107)]    0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 14, 107)     369043      ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 14, 107)      0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 14, 107)     0           ['dropout[0][0]',                \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 14, 107)     214         ['tf.__operators__.add[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 14, 107)      11556       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 14, 107)      11556       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 14, 107)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 14, 107)     0           ['dropout_1[0][0]',              \n",
      " mbda)                                                            'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 14, 107)     214         ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 107)         0           ['layer_normalization_1[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           6912        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 399,560\n",
      "Trainable params: 399,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "use_diag = 0\n",
    "use_labs = 1\n",
    "use_proc = 0\n",
    "use_meds = 0\n",
    "use_insurance = 0\n",
    "use_gender = 0\n",
    "use_race = 0\n",
    "hf = 1\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "batchSize = 256\n",
    "path='./models/labs.h5'\n",
    "\n",
    "model = Transformer(lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd10328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/254 [..............................] - ETA: 8:34 - loss: 0.8983 - auc: 0.5847"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 22:56:17.945626: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - ETA: 0s - loss: 0.3207 - auc: 0.5598\n",
      "Epoch 00001: val_loss improved from inf to 0.30101, saving model to ./models/labs.h5\n",
      "254/254 [==============================] - 73s 282ms/step - loss: 0.3207 - auc: 0.5598 - val_loss: 0.3010 - val_auc: 0.7021\n",
      "Epoch 2/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3110 - auc: 0.5910\n",
      "Epoch 00002: val_loss did not improve from 0.30101\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3110 - auc: 0.5910 - val_loss: 0.3104 - val_auc: 0.7512\n",
      "Epoch 3/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3143 - auc: 0.5956\n",
      "Epoch 00003: val_loss did not improve from 0.30101\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3143 - auc: 0.5956 - val_loss: 0.3012 - val_auc: 0.7674\n",
      "Epoch 4/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3174 - auc: 0.6069\n",
      "Epoch 00004: val_loss did not improve from 0.30101\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3174 - auc: 0.6069 - val_loss: 0.3013 - val_auc: 0.7782\n",
      "Epoch 5/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3091 - auc: 0.6195\n",
      "Epoch 00005: val_loss improved from 0.30101 to 0.29181, saving model to ./models/labs.h5\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3091 - auc: 0.6195 - val_loss: 0.2918 - val_auc: 0.8032\n",
      "Epoch 6/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3093 - auc: 0.6332\n",
      "Epoch 00006: val_loss improved from 0.29181 to 0.28667, saving model to ./models/labs.h5\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3093 - auc: 0.6332 - val_loss: 0.2867 - val_auc: 0.8098\n",
      "Epoch 7/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3100 - auc: 0.6399\n",
      "Epoch 00007: val_loss did not improve from 0.28667\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3100 - auc: 0.6399 - val_loss: 0.2876 - val_auc: 0.7854\n",
      "Epoch 8/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3027 - auc: 0.6453\n",
      "Epoch 00008: val_loss did not improve from 0.28667\n",
      "254/254 [==============================] - 71s 279ms/step - loss: 0.3027 - auc: 0.6453 - val_loss: 0.2918 - val_auc: 0.8023\n",
      "Epoch 9/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3081 - auc: 0.6426\n",
      "Epoch 00009: val_loss did not improve from 0.28667\n",
      "254/254 [==============================] - 71s 279ms/step - loss: 0.3081 - auc: 0.6426 - val_loss: 0.2890 - val_auc: 0.8115\n",
      "Epoch 10/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3054 - auc: 0.6438\n",
      "Epoch 00010: val_loss improved from 0.28667 to 0.28616, saving model to ./models/labs.h5\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3054 - auc: 0.6438 - val_loss: 0.2862 - val_auc: 0.8057\n",
      "Epoch 11/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3075 - auc: 0.6477\n",
      "Epoch 00011: val_loss did not improve from 0.28616\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3075 - auc: 0.6477 - val_loss: 0.2934 - val_auc: 0.8146\n",
      "Epoch 12/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3037 - auc: 0.6454\n",
      "Epoch 00012: val_loss improved from 0.28616 to 0.28065, saving model to ./models/labs.h5\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3037 - auc: 0.6454 - val_loss: 0.2806 - val_auc: 0.8216\n",
      "Epoch 13/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3073 - auc: 0.6492\n",
      "Epoch 00013: val_loss did not improve from 0.28065\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3073 - auc: 0.6492 - val_loss: 0.2917 - val_auc: 0.8122\n",
      "Epoch 14/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3052 - auc: 0.6511\n",
      "Epoch 00014: val_loss did not improve from 0.28065\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3052 - auc: 0.6511 - val_loss: 0.2998 - val_auc: 0.8095\n",
      "Epoch 15/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3009 - auc: 0.6533\n",
      "Epoch 00015: val_loss did not improve from 0.28065\n",
      "254/254 [==============================] - 71s 279ms/step - loss: 0.3009 - auc: 0.6533 - val_loss: 0.2968 - val_auc: 0.7991\n",
      "Epoch 16/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3094 - auc: 0.6435\n",
      "Epoch 00016: val_loss did not improve from 0.28065\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3094 - auc: 0.6435 - val_loss: 0.2919 - val_auc: 0.8086\n",
      "Epoch 17/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3077 - auc: 0.6499\n",
      "Epoch 00017: val_loss did not improve from 0.28065\n",
      "254/254 [==============================] - 71s 279ms/step - loss: 0.3077 - auc: 0.6499 - val_loss: 0.2959 - val_auc: 0.8176\n",
      "Epoch 18/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3004 - auc: 0.6567\n",
      "Epoch 00018: val_loss did not improve from 0.28065\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3004 - auc: 0.6567 - val_loss: 0.2904 - val_auc: 0.7716\n",
      "Epoch 19/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3054 - auc: 0.6545\n",
      "Epoch 00019: val_loss improved from 0.28065 to 0.27451, saving model to ./models/labs.h5\n",
      "254/254 [==============================] - 71s 281ms/step - loss: 0.3054 - auc: 0.6545 - val_loss: 0.2745 - val_auc: 0.8134\n",
      "Epoch 20/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3015 - auc: 0.6590\n",
      "Epoch 00020: val_loss did not improve from 0.27451\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3015 - auc: 0.6590 - val_loss: 0.2782 - val_auc: 0.8024\n",
      "Epoch 21/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3052 - auc: 0.6661\n",
      "Epoch 00021: val_loss improved from 0.27451 to 0.27355, saving model to ./models/labs.h5\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3052 - auc: 0.6661 - val_loss: 0.2736 - val_auc: 0.7960\n",
      "Epoch 22/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3031 - auc: 0.6540\n",
      "Epoch 00022: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 281ms/step - loss: 0.3031 - auc: 0.6540 - val_loss: 0.2870 - val_auc: 0.8053\n",
      "Epoch 23/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3045 - auc: 0.6592\n",
      "Epoch 00023: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3045 - auc: 0.6592 - val_loss: 0.2775 - val_auc: 0.8128\n",
      "Epoch 24/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3015 - auc: 0.6578\n",
      "Epoch 00024: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 279ms/step - loss: 0.3015 - auc: 0.6578 - val_loss: 0.2798 - val_auc: 0.8078\n",
      "Epoch 25/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3031 - auc: 0.6645\n",
      "Epoch 00025: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3031 - auc: 0.6645 - val_loss: 0.2852 - val_auc: 0.8023\n",
      "Epoch 26/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3000 - auc: 0.6660\n",
      "Epoch 00026: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3000 - auc: 0.6660 - val_loss: 0.2869 - val_auc: 0.8078\n",
      "Epoch 27/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3074 - auc: 0.6625\n",
      "Epoch 00027: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.3074 - auc: 0.6625 - val_loss: 0.2791 - val_auc: 0.8165\n",
      "Epoch 28/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2997 - auc: 0.6646\n",
      "Epoch 00028: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.2997 - auc: 0.6646 - val_loss: 0.2784 - val_auc: 0.8146\n",
      "Epoch 29/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2997 - auc: 0.6609\n",
      "Epoch 00029: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.2997 - auc: 0.6609 - val_loss: 0.2902 - val_auc: 0.8073\n",
      "Epoch 30/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3045 - auc: 0.6661\n",
      "Epoch 00030: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 279ms/step - loss: 0.3045 - auc: 0.6661 - val_loss: 0.2741 - val_auc: 0.8143\n",
      "Epoch 31/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2986 - auc: 0.6641\n",
      "Epoch 00031: val_loss did not improve from 0.27355\n",
      "254/254 [==============================] - 71s 280ms/step - loss: 0.2986 - auc: 0.6641 - val_loss: 0.2953 - val_auc: 0.7882\n",
      "Epoch 00031: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf)\n",
    "model.train(train_hadm, valid_hadm, epochs, batchSize, path, False, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14e3fd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 17s 202ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.810232935296863"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load(path)\n",
    "res = model.predict(test_hadm, batchSize, 1)\n",
    "y_true = cohort.set_index('hadm_id').loc[test_hadm].label_hf.to_numpy()\n",
    "roc_auc_score(y_true, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa90bd3",
   "metadata": {},
   "source": [
    "# Diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848be5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 306)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 306)      3004002     ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 306)       0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 306)      0           ['dropout[0][0]',                \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 306)      612         ['tf.__operators__.add[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 306)       93942       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 306)       93942       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 306)       0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 306)      0           ['dropout_1[0][0]',              \n",
      " mbda)                                                            'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 306)      612         ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 306)         0           ['layer_normalization_1[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           19648       ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,212,823\n",
      "Trainable params: 3,212,823\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "use_diag = 1\n",
    "use_labs = 0\n",
    "use_proc = 0\n",
    "use_meds = 0\n",
    "use_insurance = 0\n",
    "use_gender = 0\n",
    "use_race = 0\n",
    "hf = 1\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "batchSize = 256\n",
    "path='./models/diag.h5'\n",
    "\n",
    "model = Transformer(lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "795e716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3025 - auc: 0.6660\n",
      "Epoch 00001: val_loss improved from inf to 0.25810, saving model to ./models/diag.h5\n",
      "254/254 [==============================] - 40s 152ms/step - loss: 0.3025 - auc: 0.6660 - val_loss: 0.2581 - val_auc: 0.8905\n",
      "Epoch 2/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2843 - auc: 0.7310\n",
      "Epoch 00002: val_loss improved from 0.25810 to 0.24600, saving model to ./models/diag.h5\n",
      "254/254 [==============================] - 38s 152ms/step - loss: 0.2843 - auc: 0.7310 - val_loss: 0.2460 - val_auc: 0.8912\n",
      "Epoch 3/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2796 - auc: 0.7407\n",
      "Epoch 00003: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 151ms/step - loss: 0.2796 - auc: 0.7407 - val_loss: 0.2502 - val_auc: 0.8874\n",
      "Epoch 4/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2746 - auc: 0.7621\n",
      "Epoch 00004: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 151ms/step - loss: 0.2746 - auc: 0.7621 - val_loss: 0.2547 - val_auc: 0.8625\n",
      "Epoch 5/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2674 - auc: 0.7781\n",
      "Epoch 00005: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 150ms/step - loss: 0.2674 - auc: 0.7781 - val_loss: 0.2776 - val_auc: 0.8621\n",
      "Epoch 6/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2590 - auc: 0.7982\n",
      "Epoch 00006: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 150ms/step - loss: 0.2590 - auc: 0.7982 - val_loss: 0.2861 - val_auc: 0.8409\n",
      "Epoch 7/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2514 - auc: 0.8149\n",
      "Epoch 00007: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 150ms/step - loss: 0.2514 - auc: 0.8149 - val_loss: 0.2896 - val_auc: 0.8362\n",
      "Epoch 8/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2412 - auc: 0.8328\n",
      "Epoch 00008: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 150ms/step - loss: 0.2412 - auc: 0.8328 - val_loss: 0.2946 - val_auc: 0.8459\n",
      "Epoch 9/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2312 - auc: 0.8504\n",
      "Epoch 00009: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 150ms/step - loss: 0.2312 - auc: 0.8504 - val_loss: 0.3079 - val_auc: 0.8204\n",
      "Epoch 10/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2212 - auc: 0.8619\n",
      "Epoch 00010: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 150ms/step - loss: 0.2212 - auc: 0.8619 - val_loss: 0.3238 - val_auc: 0.8198\n",
      "Epoch 11/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2149 - auc: 0.8752\n",
      "Epoch 00011: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 150ms/step - loss: 0.2149 - auc: 0.8752 - val_loss: 0.3465 - val_auc: 0.8134\n",
      "Epoch 12/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2010 - auc: 0.8898\n",
      "Epoch 00012: val_loss did not improve from 0.24600\n",
      "254/254 [==============================] - 38s 150ms/step - loss: 0.2010 - auc: 0.8898 - val_loss: 0.3406 - val_auc: 0.8027\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf)\n",
    "model.train(train_hadm, valid_hadm, epochs, batchSize, path, False, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ced26ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 9s 106ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8880655034863912"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load(path)\n",
    "res = model.predict(test_hadm, batchSize, 1)\n",
    "y_true = cohort.set_index('hadm_id').loc[test_hadm].label_hf.to_numpy()\n",
    "roc_auc_score(y_true, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a826e510",
   "metadata": {},
   "source": [
    "# All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe8e0e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3, 306)]     0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 14, 107)]    0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 2, 18)]      0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 14, 55)]     0           []                               \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 3, 306)      3004002     ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 14, 107)     369043      ['input_2[0][0]',                \n",
      " eadAttention)                                                    'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 2, 18)       10818       ['input_3[0][0]',                \n",
      " eadAttention)                                                    'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 14, 55)      98175       ['input_4[0][0]',                \n",
      " eadAttention)                                                    'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3, 306)       0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 14, 107)      0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 2, 18)        0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 14, 55)       0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 3, 306)      0           ['dropout[0][0]',                \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 14, 107)     0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 2, 18)       0           ['dropout_4[0][0]',              \n",
      " mbda)                                                            'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 14, 55)      0           ['dropout_6[0][0]',              \n",
      " mbda)                                                            'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 3, 306)      612         ['tf.__operators__.add[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 14, 107)     214         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 2, 18)       36          ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 14, 55)      110         ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3, 306)       93942       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 14, 107)      11556       ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 2, 18)        342         ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 14, 55)       3080        ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3, 306)       93942       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 14, 107)      11556       ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 2, 18)        342         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 14, 55)       3080        ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 4)            20          ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 3, 306)       0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 14, 107)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 2, 18)        0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 14, 55)       0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 4)            0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 3, 306)      0           ['dropout_1[0][0]',              \n",
      " mbda)                                                            'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 14, 107)     0           ['dropout_3[0][0]',              \n",
      " mbda)                                                            'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 2, 18)       0           ['dropout_5[0][0]',              \n",
      " mbda)                                                            'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 14, 55)      0           ['dropout_7[0][0]',              \n",
      " mbda)                                                            'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 4)           8           ['dropout_8[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 3, 306)      612         ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 14, 107)     214         ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 2, 18)       36          ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 14, 55)      110         ['tf.__operators__.add_7[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 4)         0           ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 306)         0           ['layer_normalization_1[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 107)         0           ['layer_normalization_3[0][0]']  \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 18)          0           ['layer_normalization_5[0][0]']  \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 55)          0           ['layer_normalization_7[0][0]']  \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4 (Gl  (None, 4)           0           ['tf.expand_dims[0][0]']         \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 490)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_1[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'global_average_pooling1d_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_average_pooling1d_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_average_pooling1d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 64)           31424       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 1)            65          ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,733,339\n",
      "Trainable params: 3,733,339\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "use_diag = 1\n",
    "use_labs = 1\n",
    "use_proc = 1\n",
    "use_meds = 1\n",
    "use_insurance = 1\n",
    "use_gender = 1\n",
    "use_race = 1\n",
    "hf = 1\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "batchSize = 256\n",
    "path='./models/all.h5'\n",
    "\n",
    "model = Transformer(lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf)\n",
    "model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd660bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.3052 - auc: 0.6864\n",
      "Epoch 00001: val_loss improved from inf to 0.26751, saving model to ./models/all.h5\n",
      "254/254 [==============================] - 120s 462ms/step - loss: 0.3052 - auc: 0.6864 - val_loss: 0.2675 - val_auc: 0.8760\n",
      "Epoch 2/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2821 - auc: 0.7404\n",
      "Epoch 00002: val_loss did not improve from 0.26751\n",
      "254/254 [==============================] - 115s 455ms/step - loss: 0.2821 - auc: 0.7404 - val_loss: 0.2800 - val_auc: 0.8492\n",
      "Epoch 3/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2784 - auc: 0.7517\n",
      "Epoch 00003: val_loss improved from 0.26751 to 0.24858, saving model to ./models/all.h5\n",
      "254/254 [==============================] - 115s 456ms/step - loss: 0.2784 - auc: 0.7517 - val_loss: 0.2486 - val_auc: 0.8901\n",
      "Epoch 4/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2696 - auc: 0.7692\n",
      "Epoch 00004: val_loss did not improve from 0.24858\n",
      "254/254 [==============================] - 115s 455ms/step - loss: 0.2696 - auc: 0.7692 - val_loss: 0.2626 - val_auc: 0.8863\n",
      "Epoch 5/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2616 - auc: 0.7927\n",
      "Epoch 00005: val_loss improved from 0.24858 to 0.24796, saving model to ./models/all.h5\n",
      "254/254 [==============================] - 115s 456ms/step - loss: 0.2616 - auc: 0.7927 - val_loss: 0.2480 - val_auc: 0.8505\n",
      "Epoch 6/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2547 - auc: 0.8098\n",
      "Epoch 00006: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 456ms/step - loss: 0.2547 - auc: 0.8098 - val_loss: 0.2512 - val_auc: 0.8588\n",
      "Epoch 7/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2429 - auc: 0.8324\n",
      "Epoch 00007: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 455ms/step - loss: 0.2429 - auc: 0.8324 - val_loss: 0.3042 - val_auc: 0.8139\n",
      "Epoch 8/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2318 - auc: 0.8507\n",
      "Epoch 00008: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 455ms/step - loss: 0.2318 - auc: 0.8507 - val_loss: 0.3012 - val_auc: 0.8268\n",
      "Epoch 9/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2157 - auc: 0.8730\n",
      "Epoch 00009: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 455ms/step - loss: 0.2157 - auc: 0.8730 - val_loss: 0.3138 - val_auc: 0.8183\n",
      "Epoch 10/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.2034 - auc: 0.8913\n",
      "Epoch 00010: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 454ms/step - loss: 0.2034 - auc: 0.8913 - val_loss: 0.3319 - val_auc: 0.8157\n",
      "Epoch 11/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.1904 - auc: 0.9050\n",
      "Epoch 00011: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 456ms/step - loss: 0.1904 - auc: 0.9050 - val_loss: 0.3749 - val_auc: 0.7650\n",
      "Epoch 12/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.1802 - auc: 0.9159\n",
      "Epoch 00012: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 455ms/step - loss: 0.1802 - auc: 0.9159 - val_loss: 0.3919 - val_auc: 0.7567\n",
      "Epoch 13/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.1709 - auc: 0.9258\n",
      "Epoch 00013: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 455ms/step - loss: 0.1709 - auc: 0.9258 - val_loss: 0.3842 - val_auc: 0.7523\n",
      "Epoch 14/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.1610 - auc: 0.9343\n",
      "Epoch 00014: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 455ms/step - loss: 0.1610 - auc: 0.9343 - val_loss: 0.4384 - val_auc: 0.7424\n",
      "Epoch 15/100\n",
      "254/254 [==============================] - ETA: 0s - loss: 0.1502 - auc: 0.9448\n",
      "Epoch 00015: val_loss did not improve from 0.24796\n",
      "254/254 [==============================] - 115s 455ms/step - loss: 0.1502 - auc: 0.9448 - val_loss: 0.3916 - val_auc: 0.7354\n",
      "Epoch 00015: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(lr, cohort, diag, labs, proc, meds, use_diag, use_labs, use_proc, use_meds, use_insurance, use_gender, use_race, hf)\n",
    "model.train(train_hadm, valid_hadm, epochs, batchSize, path, False, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5278d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 28s 323ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84768888185124"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load(path)\n",
    "res = model.predict(test_hadm, batchSize, 1)\n",
    "y_true = cohort.set_index('hadm_id').loc[test_hadm].label_hf.to_numpy()\n",
    "roc_auc_score(y_true, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f29f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
